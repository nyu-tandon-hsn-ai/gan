{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "In this notebook, we'll be building a generative adversarial network (GAN) trained on the network flow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_inputs, get_flow_dataset, train_test_split\n",
    "from models import generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_flow_dataset()\n",
    "train, test = train_test_split(dataset, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of input image to discriminator\n",
    "input_size = 40\n",
    "# Size of latent vector to generator\n",
    "z_size = 100\n",
    "# Sizes of hidden layers in generator and discriminator\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 128\n",
    "# Leak factor for leaky ReLU\n",
    "alpha = 0.01\n",
    "# Smoothing \n",
    "smooth = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "\n",
    "Now we're building the network from the functions defined above.\n",
    "\n",
    "First is to get our inputs, `input_real, input_z` from `model_inputs` using the sizes of the input and z.\n",
    "\n",
    "Then, we'll create the generator, `generator(input_z, input_size)`. This builds the generator with the appropriate input and output sizes.\n",
    "\n",
    "Then the discriminators. We'll build two of them, one for real data and one for fake data. Since we want the weights to be the same for both real and fake data, we need to reuse the variables. For the fake data, we're getting it from the generator as `g_model`. So the real data discriminator is `discriminator(input_real)` while the fake discriminator is `discriminator(g_model, reuse=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create our input placeholders\n",
    "input_real, input_z = model_inputs(input_size, z_size)\n",
    "\n",
    "# Build the model\n",
    "g_model = generator(input_z, input_size, n_units=g_hidden_size, alpha=alpha)\n",
    "# g_model is the generator output\n",
    "\n",
    "d_model_real, d_logits_real = discriminator(input_real, n_units=d_hidden_size, alpha=alpha)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses, which is a little tricky. For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_loss_real + d_loss_fake`. The losses will by sigmoid cross-entropys, which we can get with `tf.nn.sigmoid_cross_entropy_with_logits`. We'll also wrap that in `tf.reduce_mean` to get the mean for all the images in the batch. So the losses will look something like \n",
    "\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "```\n",
    "\n",
    "For the real image logits, we'll use `d_logits_real` which we got from the discriminator in the cell above. For the labels, we want them to be all ones, since these are all real images. To help the discriminator generalize better, the labels are reduced a bit from 1.0 to 0.9, for example,  using the parameter `smooth`. This is known as label smoothing, typically used with classifiers to improve performance. In TensorFlow, it looks something like `labels = tf.ones_like(tensor) * (1 - smooth)`\n",
    "\n",
    "The discriminator loss for the fake data is similar. The logits are `d_logits_fake`, which we got from passing the generator output to the discriminator. These fake logits are used with labels of all zeros. Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "Finally, the generator losses are using `d_logits_fake`, the fake image logits. But, now the labels are all ones. The generator is trying to fool the discriminator, so it wants to discriminator to output ones for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "d_loss_real = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, \n",
    "                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                          labels=tf.zeros_like(d_logits_real)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                     labels=tf.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "We want to update the generator and discriminator variables separately. So we need to get the variables for each part build optimizers for the two parts. To get all the trainable variables, we use `tf.trainable_variables()`. This creates a list of all the variables we've defined in our graph.\n",
    "\n",
    "For the generator optimizer, we only want to generator variables. Our past selves were nice and used a variable scope to start all of our generator variable names with `generator`. So, we just need to iterate through the list from `tf.trainable_variables()` and keep variables to start with `generator`. Each variable object has an attribute `name` which holds the name of the variable as a string (`var.name == 'weights_0'` for instance). \n",
    "\n",
    "We can do something similar with the discriminator. All the variables in the discriminator start with `discriminator`.\n",
    "\n",
    "Then, in the optimizer we pass the variable lists to `var_list` in the `minimize` method. This tells the optimizer to only update the listed variables. Something like `tf.train.AdamOptimizer().minimize(loss, var_list=var_list)` will only train the variables in `var_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Get the trainable_variables, split into G and D parts\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200... Discriminator Loss: 0.3389... Generator Loss: 5.9586\n",
      "Epoch 2/200... Discriminator Loss: 0.3293... Generator Loss: 7.7626\n",
      "Epoch 3/200... Discriminator Loss: 0.3290... Generator Loss: 7.1964\n",
      "Epoch 4/200... Discriminator Loss: 0.3282... Generator Loss: 6.8510\n",
      "Epoch 5/200... Discriminator Loss: 0.3269... Generator Loss: 10.6777\n",
      "Epoch 6/200... Discriminator Loss: 0.3267... Generator Loss: 11.0285\n",
      "Epoch 7/200... Discriminator Loss: 0.3267... Generator Loss: 8.7173\n",
      "Epoch 8/200... Discriminator Loss: 0.3274... Generator Loss: 6.8933\n",
      "Epoch 9/200... Discriminator Loss: 0.3263... Generator Loss: 10.7104\n",
      "Epoch 10/200... Discriminator Loss: 0.3288... Generator Loss: 12.8962\n",
      "Epoch 11/200... Discriminator Loss: 0.3262... Generator Loss: 10.1557\n",
      "Epoch 12/200... Discriminator Loss: 0.3264... Generator Loss: 13.4282\n",
      "Epoch 13/200... Discriminator Loss: 0.3256... Generator Loss: 11.4605\n",
      "Epoch 14/200... Discriminator Loss: 0.3269... Generator Loss: 7.1222\n",
      "Epoch 15/200... Discriminator Loss: 0.3255... Generator Loss: 9.9242\n",
      "Epoch 16/200... Discriminator Loss: 0.3255... Generator Loss: 11.7959\n",
      "Epoch 17/200... Discriminator Loss: 0.3252... Generator Loss: 13.0151\n",
      "Epoch 18/200... Discriminator Loss: 0.3251... Generator Loss: 13.4970\n",
      "Epoch 19/200... Discriminator Loss: 0.3251... Generator Loss: 13.8313\n",
      "Epoch 20/200... Discriminator Loss: 0.3251... Generator Loss: 14.0827\n",
      "Epoch 21/200... Discriminator Loss: 0.3251... Generator Loss: 14.2943\n",
      "Epoch 22/200... Discriminator Loss: 0.3251... Generator Loss: 14.4814\n",
      "Epoch 23/200... Discriminator Loss: 0.3251... Generator Loss: 14.6445\n",
      "Epoch 24/200... Discriminator Loss: 0.3251... Generator Loss: 14.8024\n",
      "Epoch 25/200... Discriminator Loss: 0.3251... Generator Loss: 14.9569\n",
      "Epoch 26/200... Discriminator Loss: 0.3251... Generator Loss: 15.1133\n",
      "Epoch 27/200... Discriminator Loss: 0.3251... Generator Loss: 15.2707\n",
      "Epoch 28/200... Discriminator Loss: 0.3251... Generator Loss: 15.4111\n",
      "Epoch 29/200... Discriminator Loss: 0.3251... Generator Loss: 15.5514\n",
      "Epoch 30/200... Discriminator Loss: 0.3251... Generator Loss: 15.6750\n",
      "Epoch 31/200... Discriminator Loss: 0.3251... Generator Loss: 15.7827\n",
      "Epoch 32/200... Discriminator Loss: 0.3251... Generator Loss: 15.8920\n",
      "Epoch 33/200... Discriminator Loss: 0.3251... Generator Loss: 16.0007\n",
      "Epoch 34/200... Discriminator Loss: 0.3251... Generator Loss: 16.1102\n",
      "Epoch 35/200... Discriminator Loss: 0.3251... Generator Loss: 16.2140\n",
      "Epoch 36/200... Discriminator Loss: 0.3251... Generator Loss: 16.3272\n",
      "Epoch 37/200... Discriminator Loss: 0.3251... Generator Loss: 16.4389\n",
      "Epoch 38/200... Discriminator Loss: 0.3251... Generator Loss: 16.5400\n",
      "Epoch 39/200... Discriminator Loss: 0.3251... Generator Loss: 16.6431\n",
      "Epoch 40/200... Discriminator Loss: 0.3251... Generator Loss: 16.7505\n",
      "Epoch 41/200... Discriminator Loss: 0.3251... Generator Loss: 16.8599\n",
      "Epoch 42/200... Discriminator Loss: 0.3251... Generator Loss: 16.9595\n",
      "Epoch 43/200... Discriminator Loss: 0.3251... Generator Loss: 17.0566\n",
      "Epoch 44/200... Discriminator Loss: 0.3251... Generator Loss: 17.1457\n",
      "Epoch 45/200... Discriminator Loss: 0.3251... Generator Loss: 17.2428\n",
      "Epoch 46/200... Discriminator Loss: 0.3251... Generator Loss: 17.3464\n",
      "Epoch 47/200... Discriminator Loss: 0.3251... Generator Loss: 17.4552\n",
      "Epoch 48/200... Discriminator Loss: 0.3251... Generator Loss: 17.5549\n",
      "Epoch 49/200... Discriminator Loss: 0.3251... Generator Loss: 17.6045\n",
      "Epoch 50/200... Discriminator Loss: 0.3251... Generator Loss: 17.7460\n",
      "Epoch 51/200... Discriminator Loss: 0.3251... Generator Loss: 17.8104\n",
      "Epoch 52/200... Discriminator Loss: 0.3251... Generator Loss: 17.9081\n",
      "Epoch 53/200... Discriminator Loss: 0.3251... Generator Loss: 17.9138\n",
      "Epoch 54/200... Discriminator Loss: 0.3251... Generator Loss: 18.0506\n",
      "Epoch 55/200... Discriminator Loss: 0.3251... Generator Loss: 18.1607\n",
      "Epoch 56/200... Discriminator Loss: 0.3251... Generator Loss: 18.2311\n",
      "Epoch 57/200... Discriminator Loss: 0.3251... Generator Loss: 18.3204\n",
      "Epoch 58/200... Discriminator Loss: 0.3251... Generator Loss: 18.3873\n",
      "Epoch 59/200... Discriminator Loss: 0.3251... Generator Loss: 18.5617\n",
      "Epoch 60/200... Discriminator Loss: 0.3251... Generator Loss: 18.5265\n",
      "Epoch 61/200... Discriminator Loss: 0.3251... Generator Loss: 18.6089\n",
      "Epoch 62/200... Discriminator Loss: 0.3251... Generator Loss: 18.7016\n",
      "Epoch 63/200... Discriminator Loss: 0.3251... Generator Loss: 18.7938\n",
      "Epoch 64/200... Discriminator Loss: 0.3251... Generator Loss: 18.8629\n",
      "Epoch 65/200... Discriminator Loss: 0.3251... Generator Loss: 18.9097\n",
      "Epoch 66/200... Discriminator Loss: 0.3251... Generator Loss: 19.0005\n",
      "Epoch 67/200... Discriminator Loss: 0.3251... Generator Loss: 19.0655\n",
      "Epoch 68/200... Discriminator Loss: 0.3251... Generator Loss: 19.1189\n",
      "Epoch 69/200... Discriminator Loss: 0.3251... Generator Loss: 19.2152\n",
      "Epoch 70/200... Discriminator Loss: 0.3251... Generator Loss: 19.3138\n",
      "Epoch 71/200... Discriminator Loss: 0.3251... Generator Loss: 19.3629\n",
      "Epoch 72/200... Discriminator Loss: 0.3251... Generator Loss: 19.2864\n",
      "Epoch 73/200... Discriminator Loss: 0.3251... Generator Loss: 19.4555\n",
      "Epoch 74/200... Discriminator Loss: 0.3251... Generator Loss: 19.3980\n",
      "Epoch 75/200... Discriminator Loss: 0.3251... Generator Loss: 19.5159\n",
      "Epoch 76/200... Discriminator Loss: 0.3251... Generator Loss: 19.5845\n",
      "Epoch 77/200... Discriminator Loss: 0.3251... Generator Loss: 19.6472\n",
      "Epoch 78/200... Discriminator Loss: 0.3251... Generator Loss: 19.7056\n",
      "Epoch 79/200... Discriminator Loss: 0.3251... Generator Loss: 19.7658\n",
      "Epoch 80/200... Discriminator Loss: 0.3251... Generator Loss: 19.8437\n",
      "Epoch 81/200... Discriminator Loss: 0.3251... Generator Loss: 19.7456\n",
      "Epoch 82/200... Discriminator Loss: 0.3251... Generator Loss: 19.8178\n",
      "Epoch 83/200... Discriminator Loss: 0.3251... Generator Loss: 20.0012\n",
      "Epoch 84/200... Discriminator Loss: 0.3251... Generator Loss: 19.7674\n",
      "Epoch 85/200... Discriminator Loss: 0.3251... Generator Loss: 19.8014\n",
      "Epoch 86/200... Discriminator Loss: 0.3251... Generator Loss: 19.8323\n",
      "Epoch 87/200... Discriminator Loss: 0.3251... Generator Loss: 19.8783\n",
      "Epoch 88/200... Discriminator Loss: 0.3251... Generator Loss: 19.9322\n",
      "Epoch 89/200... Discriminator Loss: 0.3251... Generator Loss: 19.6694\n",
      "Epoch 90/200... Discriminator Loss: 0.3251... Generator Loss: 19.6943\n",
      "Epoch 91/200... Discriminator Loss: 0.3251... Generator Loss: 19.8635\n",
      "Epoch 92/200... Discriminator Loss: 0.3251... Generator Loss: 20.0642\n",
      "Epoch 93/200... Discriminator Loss: 0.3251... Generator Loss: 20.0488\n",
      "Epoch 94/200... Discriminator Loss: 0.3251... Generator Loss: 20.1057\n",
      "Epoch 95/200... Discriminator Loss: 0.3251... Generator Loss: 19.4325\n",
      "Epoch 96/200... Discriminator Loss: 0.3251... Generator Loss: 19.6618\n",
      "Epoch 97/200... Discriminator Loss: 0.3251... Generator Loss: 20.0875\n",
      "Epoch 98/200... Discriminator Loss: 0.3251... Generator Loss: 20.3806\n",
      "Epoch 99/200... Discriminator Loss: 0.3251... Generator Loss: 20.3766\n",
      "Epoch 100/200... Discriminator Loss: 0.3251... Generator Loss: 20.3681\n",
      "Epoch 101/200... Discriminator Loss: 0.3251... Generator Loss: 20.4697\n",
      "Epoch 102/200... Discriminator Loss: 0.3251... Generator Loss: 20.4461\n",
      "Epoch 103/200... Discriminator Loss: 0.3251... Generator Loss: 19.4521\n",
      "Epoch 104/200... Discriminator Loss: 0.3251... Generator Loss: 20.1480\n",
      "Epoch 105/200... Discriminator Loss: 0.3251... Generator Loss: 20.5371\n",
      "Epoch 106/200... Discriminator Loss: 0.3251... Generator Loss: 19.9610\n",
      "Epoch 107/200... Discriminator Loss: 0.3251... Generator Loss: 15.9671\n",
      "Epoch 108/200... Discriminator Loss: 0.3251... Generator Loss: 20.6103\n",
      "Epoch 109/200... Discriminator Loss: 0.3251... Generator Loss: 21.0427\n",
      "Epoch 110/200... Discriminator Loss: 0.3251... Generator Loss: 21.1673\n",
      "Epoch 111/200... Discriminator Loss: 0.3251... Generator Loss: 21.1684\n",
      "Epoch 112/200... Discriminator Loss: 0.3251... Generator Loss: 21.0617\n",
      "Epoch 113/200... Discriminator Loss: 0.3251... Generator Loss: 20.8396\n",
      "Epoch 114/200... Discriminator Loss: 0.3251... Generator Loss: 20.5236\n",
      "Epoch 115/200... Discriminator Loss: 0.3251... Generator Loss: 20.7445\n",
      "Epoch 116/200... Discriminator Loss: 0.3251... Generator Loss: 21.1164\n",
      "Epoch 117/200... Discriminator Loss: 0.3251... Generator Loss: 20.5065\n",
      "Epoch 118/200... Discriminator Loss: 0.3251... Generator Loss: 20.9284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200... Discriminator Loss: 0.3251... Generator Loss: 20.8689\n",
      "Epoch 120/200... Discriminator Loss: 0.3251... Generator Loss: 20.5581\n",
      "Epoch 121/200... Discriminator Loss: 0.3251... Generator Loss: 20.9778\n",
      "Epoch 122/200... Discriminator Loss: 0.3251... Generator Loss: 20.8783\n",
      "Epoch 123/200... Discriminator Loss: 0.3251... Generator Loss: 20.7351\n",
      "Epoch 124/200... Discriminator Loss: 0.3251... Generator Loss: 21.1605\n",
      "Epoch 125/200... Discriminator Loss: 0.3251... Generator Loss: 21.6736\n",
      "Epoch 126/200... Discriminator Loss: 0.3251... Generator Loss: 17.6213\n",
      "Epoch 127/200... Discriminator Loss: 0.3255... Generator Loss: 12.9143\n",
      "Epoch 128/200... Discriminator Loss: 0.3251... Generator Loss: 16.5004\n",
      "Epoch 129/200... Discriminator Loss: 0.3251... Generator Loss: 17.2150\n",
      "Epoch 130/200... Discriminator Loss: 0.3251... Generator Loss: 17.8838\n",
      "Epoch 131/200... Discriminator Loss: 0.3251... Generator Loss: 18.2379\n",
      "Epoch 132/200... Discriminator Loss: 0.3251... Generator Loss: 18.5533\n",
      "Epoch 133/200... Discriminator Loss: 0.3251... Generator Loss: 18.8339\n",
      "Epoch 134/200... Discriminator Loss: 0.3251... Generator Loss: 19.0363\n",
      "Epoch 135/200... Discriminator Loss: 0.3252... Generator Loss: 14.0126\n",
      "Epoch 136/200... Discriminator Loss: 0.3251... Generator Loss: 15.1572\n",
      "Epoch 137/200... Discriminator Loss: 0.3251... Generator Loss: 15.0869\n",
      "Epoch 138/200... Discriminator Loss: 0.3251... Generator Loss: 15.8967\n",
      "Epoch 139/200... Discriminator Loss: 0.3251... Generator Loss: 16.3047\n",
      "Epoch 140/200... Discriminator Loss: 0.3251... Generator Loss: 16.6229\n",
      "Epoch 141/200... Discriminator Loss: 0.3251... Generator Loss: 16.9122\n",
      "Epoch 142/200... Discriminator Loss: 0.3251... Generator Loss: 17.1424\n",
      "Epoch 143/200... Discriminator Loss: 0.3251... Generator Loss: 17.3313\n",
      "Epoch 144/200... Discriminator Loss: 0.3251... Generator Loss: 17.4994\n",
      "Epoch 145/200... Discriminator Loss: 0.3251... Generator Loss: 17.6542\n",
      "Epoch 146/200... Discriminator Loss: 0.3251... Generator Loss: 17.8169\n",
      "Epoch 147/200... Discriminator Loss: 0.3251... Generator Loss: 17.9439\n",
      "Epoch 148/200... Discriminator Loss: 0.3251... Generator Loss: 18.0780\n",
      "Epoch 149/200... Discriminator Loss: 0.3251... Generator Loss: 18.2089\n",
      "Epoch 150/200... Discriminator Loss: 0.3251... Generator Loss: 18.3327\n",
      "Epoch 151/200... Discriminator Loss: 0.3251... Generator Loss: 18.4471\n",
      "Epoch 152/200... Discriminator Loss: 0.3251... Generator Loss: 18.5532\n",
      "Epoch 153/200... Discriminator Loss: 0.3251... Generator Loss: 18.6698\n",
      "Epoch 154/200... Discriminator Loss: 0.3251... Generator Loss: 18.7814\n",
      "Epoch 155/200... Discriminator Loss: 0.3251... Generator Loss: 18.8814\n",
      "Epoch 156/200... Discriminator Loss: 0.3251... Generator Loss: 18.9123\n",
      "Epoch 157/200... Discriminator Loss: 0.3251... Generator Loss: 19.0741\n",
      "Epoch 158/200... Discriminator Loss: 0.3251... Generator Loss: 19.1734\n",
      "Epoch 159/200... Discriminator Loss: 0.3251... Generator Loss: 19.2288\n",
      "Epoch 160/200... Discriminator Loss: 0.3251... Generator Loss: 19.2748\n",
      "Epoch 161/200... Discriminator Loss: 0.3251... Generator Loss: 19.2881\n",
      "Epoch 162/200... Discriminator Loss: 0.3251... Generator Loss: 19.4539\n",
      "Epoch 163/200... Discriminator Loss: 0.3251... Generator Loss: 19.5029\n",
      "Epoch 164/200... Discriminator Loss: 0.3251... Generator Loss: 19.5953\n",
      "Epoch 165/200... Discriminator Loss: 0.3251... Generator Loss: 18.9378\n",
      "Epoch 166/200... Discriminator Loss: 0.3251... Generator Loss: 19.2399\n",
      "Epoch 167/200... Discriminator Loss: 0.3251... Generator Loss: 19.3359\n",
      "Epoch 168/200... Discriminator Loss: 0.3251... Generator Loss: 19.4525\n",
      "Epoch 169/200... Discriminator Loss: 0.3251... Generator Loss: 19.5160\n",
      "Epoch 170/200... Discriminator Loss: 0.3251... Generator Loss: 19.4399\n",
      "Epoch 171/200... Discriminator Loss: 0.3251... Generator Loss: 19.7127\n",
      "Epoch 172/200... Discriminator Loss: 0.3251... Generator Loss: 19.8907\n",
      "Epoch 173/200... Discriminator Loss: 0.3251... Generator Loss: 19.0374\n",
      "Epoch 174/200... Discriminator Loss: 0.3251... Generator Loss: 18.5184\n",
      "Epoch 175/200... Discriminator Loss: 0.3251... Generator Loss: 19.1006\n",
      "Epoch 176/200... Discriminator Loss: 0.3251... Generator Loss: 19.5600\n",
      "Epoch 177/200... Discriminator Loss: 0.3251... Generator Loss: 19.7577\n",
      "Epoch 178/200... Discriminator Loss: 0.3251... Generator Loss: 19.8437\n",
      "Epoch 179/200... Discriminator Loss: 0.3251... Generator Loss: 20.2067\n",
      "Epoch 180/200... Discriminator Loss: 0.3251... Generator Loss: 19.7302\n",
      "Epoch 181/200... Discriminator Loss: 0.3251... Generator Loss: 19.9757\n",
      "Epoch 182/200... Discriminator Loss: 0.3251... Generator Loss: 20.3492\n",
      "Epoch 183/200... Discriminator Loss: 0.3251... Generator Loss: 20.6609\n",
      "Epoch 184/200... Discriminator Loss: 0.3251... Generator Loss: 20.5618\n",
      "Epoch 185/200... Discriminator Loss: 0.3251... Generator Loss: 20.6192\n",
      "Epoch 186/200... Discriminator Loss: 0.3251... Generator Loss: 20.3944\n",
      "Epoch 187/200... Discriminator Loss: 0.3251... Generator Loss: 20.7810\n",
      "Epoch 188/200... Discriminator Loss: 0.3251... Generator Loss: 20.3166\n",
      "Epoch 189/200... Discriminator Loss: 0.3251... Generator Loss: 20.3506\n",
      "Epoch 190/200... Discriminator Loss: 0.3251... Generator Loss: 20.6256\n",
      "Epoch 191/200... Discriminator Loss: 0.3251... Generator Loss: 20.0454\n",
      "Epoch 192/200... Discriminator Loss: 0.3251... Generator Loss: 20.4082\n",
      "Epoch 193/200... Discriminator Loss: 0.3251... Generator Loss: 20.7738\n",
      "Epoch 194/200... Discriminator Loss: 0.3251... Generator Loss: 20.7292\n",
      "Epoch 195/200... Discriminator Loss: 0.3251... Generator Loss: 20.9505\n",
      "Epoch 196/200... Discriminator Loss: 0.3251... Generator Loss: 20.4057\n",
      "Epoch 197/200... Discriminator Loss: 0.3251... Generator Loss: 19.4859\n",
      "Epoch 198/200... Discriminator Loss: 0.3251... Generator Loss: 20.6967\n",
      "Epoch 199/200... Discriminator Loss: 0.3251... Generator Loss: 21.2903\n",
      "Epoch 200/200... Discriminator Loss: 0.3251... Generator Loss: 21.6456\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 200\n",
    "samples = []\n",
    "losses = []\n",
    "# Only save generator variables\n",
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(train.shape[0]//batch_size):\n",
    "            batch_real = train[ii * batch_size:(ii + 1) * batch_size]\n",
    "            \n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: batch_real, input_z: batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})\n",
    "        \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_real})\n",
    "        train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))    \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                       generator(input_z, input_size, reuse=True),\n",
    "                       feed_dict={input_z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "Here we'll check out the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b895ac7a7b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator')\n",
    "plt.plot(losses.T[1], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
