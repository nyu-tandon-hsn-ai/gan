{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "In this notebook, we'll be building a generative adversarial network (GAN) trained on the network flow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, models\n",
    "reload(utils)\n",
    "reload(models)\n",
    "from models import GeneratorModel, DiscriminatorModel\n",
    "from utils import max_norm, parse_feature_label, train_test_split, loss, grad, sample_n_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some constants\n",
    "IDS_DATASET = os.path.join('data', 'ids2017_sampled.csv')\n",
    "RELEVANT_FEATURES = [' Source Port', ' Destination Port', ' Flow Duration', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', 'Bwd Packet Length Max', ' Bwd Packet Length Min', 'Flow Bytes/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Fwd Header Length', ' Bwd Packets/s', ' Packet Length Mean', ' ACK Flag Count', ' Down/Up Ratio', ' Avg Fwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' act_data_pkt_fwd', ' Active Std', ' Active Min', ' Idle Max']\n",
    "LABEL_NAME = ' Label'\n",
    "BENIGN_LABEL = 0\n",
    "ATTACK_LABEL = 2\n",
    "TRAIN_FRAC = 0.3\n",
    "FEATURE_NUM_MODIFIED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow ID, Source IP, Source Port, Destination IP, Destination Port, Protocol, Timestamp, Flow Duration, Total Fwd Packets, Total Backward Packets,Total Length of Fwd Packets, Total Length of Bwd Packets, Fwd Packet Length Max, Fwd Packet Length Min, Fwd Packet Length Mean, Fwd Packet Length Std,Bwd Packet Length Max, Bwd Packet Length Min, Bwd Packet Length Mean, Bwd Packet Length Std,Flow Bytes/s, Flow Packets/s, Flow IAT Mean, Flow IAT Std, Flow IAT Max, Flow IAT Min,Fwd IAT Total, Fwd IAT Mean, Fwd IAT Std, Fwd IAT Max, Fwd IAT Min,Bwd IAT Total, Bwd IAT Mean, Bwd IAT Std, Bwd IAT Max, Bwd IAT Min,Fwd PSH Flags, Bwd PSH Flags, Fwd URG Flags, Bwd URG Flags, Fwd Header Length, Bwd Header Length,Fwd Packets/s, Bwd Packets/s, Min Packet Length, Max Packet Length, Packet Length Mean, Packet Length Std, Packet Length Variance,FIN Flag Count, SYN Flag Count, RST Flag Count, PSH Flag Count, ACK Flag Count, URG Flag Count, CWE Flag Count, ECE Flag Count, Down/Up Ratio, Average Packet Size, Avg Fwd Segment Size, Avg Bwd Segment Size, Fwd Header Length.1,Fwd Avg Bytes/Bulk, Fwd Avg Packets/Bulk, Fwd Avg Bulk Rate, Bwd Avg Bytes/Bulk, Bwd Avg Packets/Bulk,Bwd Avg Bulk Rate,Subflow Fwd Packets, Subflow Fwd Bytes, Subflow Bwd Packets, Subflow Bwd Bytes,Init_Win_bytes_forward, Init_Win_bytes_backward, act_data_pkt_fwd, min_seg_size_forward,Active Mean, Active Std, Active Max, Active Min,Idle Mean, Idle Std, Idle Max, Idle Min, Label\r\n",
      "214102,192.168.10.8,50305,23.194.108.67,80,6,5/7/2017 9:35,5559809,3,1,12,0.0,6,0,4.0,3.464101615,0,0,0.0,0.0,2.158347526,0.719449175,1853269.667,3189322.073,5535956.0,39.0,5559809.0,2779904.5,3897645.41,5535956.0,23853.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,72,32,0.539586881,0.179862294,0,6,2.4,3.286335345,10.8,0,0,0,1,0,0,0,0,0,3.0,4.0,0.0,72,0,0,0,0,0,0,3,12,1,0,8192,29200,2,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "36502,172.217.11.3,80,192.168.10.8,49917,6,5/7/2017 9:31,18,1,1,6,6.0,6,6,6.0,0.0,6,6,6.0,0.0,666666.6667,111111.1111,18.0,0.0,18.0,18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,20,20,55555.55556,55555.55556,6,6,6.0,0.0,0.0,0,0,0,0,1,1,0,0,1,9.0,6.0,6.0,20,0,0,0,0,0,0,1,6,1,6,343,16560,0,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "24975,192.168.10.50,80,172.16.0.1,51484,6,5/7/2017 11:00,12859,1,3,0,18.0,0,0,0.0,0.0,6,6,6.0,0.0,1399.797807,311.0661793,4286.333333,6734.794825,12049.0,1.0,0.0,0.0,0.0,0.0,0.0,12050.0,6025.0,8519.2225,12049.0,1.0,0,0,0,0,32,60,77.76654483,233.2996345,0,6,3.6,3.286335345,10.8,0,0,0,0,1,0,0,0,3,4.5,0.0,6.0,32,0,0,0,0,0,0,1,0,3,18,235,0,0,32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "151291,192.168.10.19,27610,192.168.10.3,53,17,5/7/2017 3:08,231,2,2,62,94.0,31,31,31.0,0.0,47,47,47.0,0.0,675324.6753,17316.01732,77.0,91.27978966,179.0,3.0,3.0,3.0,0.0,3.0,3.0,49.0,49.0,0.0,49.0,49.0,0,0,0,0,40,40,8658.008658,8658.008658,31,47,37.4,8.76356092,76.8,0,0,0,0,0,0,0,0,1,46.75,31.0,47.0,40,0,0,0,0,0,0,2,62,2,94,-1,-1,1,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n"
     ]
    }
   ],
   "source": [
    "# quick view of dataset\n",
    "!head -n5 {IDS_DATASET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(IDS_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant features and label name\n",
    "df = df[RELEVANT_FEATURES + [LABEL_NAME]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bengin and attack flows we want\n",
    "benign_df, attack_df = df[(df[LABEL_NAME] == BENIGN_LABEL)], df[(df[LABEL_NAME] == ATTACK_LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# rewrite label values\n",
    "benign_df.loc[:, LABEL_NAME] = 0\n",
    "attack_df.loc[:, LABEL_NAME] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "benign, attack = benign_df.values, attack_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max normalization\n",
    "benign[:, :len(RELEVANT_FEATURES)] = max_norm(benign[:, :len(RELEVANT_FEATURES)])\n",
    "attack[:, :len(RELEVANT_FEATURES)] = max_norm(attack[:, :len(RELEVANT_FEATURES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# do train, test split separately on benign and attack\n",
    "benign_train, benign_test = train_test_split(benign, train_size=TRAIN_FRAC)\n",
    "attack_train, attack_test = train_test_split(attack, train_size=TRAIN_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat to get training and testing data\n",
    "train_np, test_np = np.concatenate([benign_train, attack_train]), np.concatenate([benign_test, attack_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example features: tf.Tensor(\n",
      "[9.54503831e-01 8.26536500e-04 5.09829282e-04 2.95782147e-04\n",
      " 3.36898396e-06 1.61815068e-02 3.25301205e-01 1.19560013e-02\n",
      " 9.54321358e-04 0.00000000e+00 5.18406775e-04 9.54321358e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.50946146e-05\n",
      " 1.63476157e-05 7.03479576e-02 0.00000000e+00 1.42857143e-01\n",
      " 1.94049159e-02 8.50946146e-05 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.95782147e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00], shape=(41,), dtype=float64)\n",
      "example label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# convert to dataset\n",
    "train_dataset, test_dataset = tf.data.Dataset.from_tensor_slices(train_np), tf.data.Dataset.from_tensor_slices(test_np)\n",
    "\n",
    "train_dataset = train_dataset.map(parse_feature_label)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_np.shape[0] * 5)  # randomize\n",
    "train_dataset = train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "features, label = iter(train_dataset).next()\n",
    "print(\"example features:\", features[0])\n",
    "print(\"example label:\", label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = GeneratorModel()\n",
    "d_model = DiscriminatorModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tfe.metrics.Mean()\n",
    "    epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "#     for x, y in train_dataset:\n",
    "#         if y == 0:\n",
    "#             # Optimize the model\n",
    "#             grads = grad(model, x, y)\n",
    "#             optimizer.apply_gradients(\n",
    "#                 zip(grads, model.variables),\n",
    "#                 global_step=tf.train.get_or_create_global_step()\n",
    "#             )\n",
    "#         elif y == 1:\n",
    "#             feature_nums = sample_n_number(len(RELEVANT_FEATURES), FEATURE_NUM_MODIFIED)\n",
    "#             z = x[:, feature_nums]\n",
    "#             modified_x = \n",
    "#         else:\n",
    "#             raise AssertionError()\n",
    "\n",
    "#         # Track progress\n",
    "#         epoch_loss_avg(loss(model, x, y))  # add current batch loss\n",
    "#         # compare predicted label to actual label\n",
    "#         epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
    "\n",
    "#     # end epoch\n",
    "#     train_loss_results.append(epoch_loss_avg.result())\n",
    "#     train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "#     if epoch % 50 == 0:\n",
    "#         print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(\n",
    "#                 epoch,\n",
    "#                 epoch_loss_avg.result(),\n",
    "#                 epoch_accuracy.result()\n",
    "#             )\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of input flow to discriminator\n",
    "input_size = benign_train.shape[1]\n",
    "# Size of latent vector to generator\n",
    "z_size = 2\n",
    "# Sizes of hidden layers in generator and discriminator\n",
    "g_hidden_size = [128, 64]\n",
    "d_hidden_size = [128, 64]\n",
    "# Leak factor for leaky ReLU\n",
    "alpha = 0.01\n",
    "# log interval\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "\n",
    "Now we're building the network from the functions defined above.\n",
    "\n",
    "First is to get our inputs, `input_benign, input_z, input_attack_remains` from `model_inputs` using the sizes of the input and z.\n",
    "\n",
    "Then, we'll create the generator, `generator(input_z, z_size)`. This builds the generator with the appropriate input and output sizes.\n",
    "\n",
    "Then the discriminators. We'll build two of them, one for benign flow data and one for attack flow data. Since we want the weights to be the same for both benign and attack flow data, we need to reuse the variables. For the attack flow data, we're getting it from the output of the generator concatenated with remaining part of attack feature called `g_model`. So the benign data discriminator is `discriminator(input_benign)` while the attack discriminator is `discriminator(g_model, reuse=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create our input placeholders\n",
    "input_benign, input_z, input_attack_remains = model_inputs(input_size, z_size, attack_remains_dim=input_size - z_size)\n",
    "\n",
    "# Build the model\n",
    "z_generated, g_hidden = generator(input_z, z_size, n_units=[z_size] + g_hidden_size, alpha=alpha)\n",
    "g_model = tf.concat([z_generated, input_attack_remains], 1)\n",
    "# g_model is the generator output concatenated with the remaining part of attack features\n",
    "\n",
    "d_model_benign, d_logits_benign, d_hidden_benign = discriminator(input_benign,\n",
    "                                                                 n_units= [input_size] + d_hidden_size,\n",
    "                                                                 alpha=alpha)\n",
    "d_model_attack, d_logits_attack, d_hidden_attack = discriminator(g_model,\n",
    "                                                                 reuse=True,\n",
    "                                                                 n_units=[input_size] + d_hidden_size,\n",
    "                                                                 alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses, which is a little tricky. For the discriminator, the total loss is the sum of the losses for benign and attack flows, `d_loss = d_loss_benign + d_loss_attack`. The losses will by sigmoid cross-entropys, which we can get with `tf.nn.sigmoid_cross_entropy_with_logits`. We'll also wrap that in `tf.reduce_mean` to get the mean for all the flows in the batch. So the losses will look something like \n",
    "\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "```\n",
    "\n",
    "For the benign flow logits, we'll use `d_logits_benign` which we got from the discriminator in the cell above. For the labels, we want them to be all zeros, since these are all benign flows. In TensorFlow, it looks something like `labels = tf.zeros_like(tensor)`\n",
    "\n",
    "The discriminator loss for the attack flow data is similar. The logits are `d_logits_attack`, which we got from passing the generator output concatenated with remaing part of attack flow features to the discriminator. These attack logits are used with labels of all ones. Remember that we want the discriminator to output 0 for benign flows and 1 for attack flows, so we need to set up the losses to reflect that.\n",
    "\n",
    "Finally, the generator losses are using `d_logits_attack`, the attack flow logits. But, now the labels are all zeros. The generator is trying to fool the discriminator, so it wants to discriminator to output zeros for attack flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "d_loss_benign = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_benign, \n",
    "                                                          labels=tf.zeros_like(d_logits_benign)))\n",
    "d_loss_attack = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_attack, \n",
    "                                                          labels=tf.ones_like(d_logits_attack)))\n",
    "d_loss = d_loss_benign + d_loss_attack\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_attack,\n",
    "                                                     labels=tf.zeros_like(d_logits_attack)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "We want to update the generator and discriminator variables separately. So we need to get the variables for each part build optimizers for the two parts. To get all the trainable variables, we use `tf.trainable_variables()`. This creates a list of all the variables we've defined in our graph.\n",
    "\n",
    "For the generator optimizer, we only want to generator variables. Our past selves were nice and used a variable scope to start all of our generator variable names with `generator`. So, we just need to iterate through the list from `tf.trainable_variables()` and keep variables to start with `generator`. Each variable object has an attribute `name` which holds the name of the variable as a string (`var.name == 'weights_0'` for instance). \n",
    "\n",
    "We can do something similar with the discriminator. All the variables in the discriminator start with `discriminator`.\n",
    "\n",
    "Then, in the optimizer we pass the variable lists to `var_list` in the `minimize` method. This tells the optimizer to only update the listed variables. Something like `tf.train.AdamOptimizer().minimize(loss, var_list=var_list)` will only train the variables in `var_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Get the trainable_variables, split into G and D parts\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 1000\n",
    "losses = []\n",
    "original_flows, generated_flows, attack_scores = [], [], []\n",
    "# Only save generator variables\n",
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "# TODO: add outer-loop, which is test number, to see convergence trends\n",
    "# TODO: modify training process - choose random 2 features and give 2 outputs randomly back\n",
    "with tf.Session() as sess:\n",
    "    total_start = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    init_benign_test_score = sess.run(d_model_benign, feed_dict={input_benign: benign_test})\n",
    "    init_attack_test_score = sess.run(d_model_benign, feed_dict={input_benign: attack_test})\n",
    "    for e in range(epochs):\n",
    "        start = time.time()\n",
    "        np.random.shuffle(benign_train)\n",
    "        np.random.shuffle(attack_train)\n",
    "        original_flows.append([])\n",
    "        generated_flows.append([])\n",
    "        attack_scores.append([])\n",
    "        for ii in range(benign_train.shape[0]//batch_size):\n",
    "            batch_benign = benign_train[ii * batch_size:(ii + 1) * batch_size]\n",
    "            \n",
    "            batch_attack = attack_train[ii * batch_size:(ii + 1) * batch_size]\n",
    "            batch_z = batch_attack[:,:z_size]\n",
    "            batch_attack_remains = batch_attack[:,z_size:]\n",
    "            \n",
    "#             if ii % log_interval == 0:\n",
    "#                 original_flow = batch_attack\n",
    "#                 generated_flow = sess.run(g_model, feed_dict={input_z: batch_z, input_attack_remains: batch_attack_remains})\n",
    "#                 attack_score = sess.run(d_model_attack, feed_dict={input_z: batch_z, input_attack_remains: batch_attack_remains})\n",
    "#                 original_flows[-1].append(deepcopy(original_flow[0]))\n",
    "#                 generated_flows[-1].append(generated_flow[0])\n",
    "#                 attack_scores[-1].append(attack_score[0][0])\n",
    "                \n",
    "            \n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_benign: batch_benign, input_z: batch_z, input_attack_remains: batch_attack_remains})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z, input_attack_remains: batch_attack_remains})\n",
    "        \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_benign: batch_benign, input_attack_remains: batch_attack_remains})\n",
    "        train_loss_g = g_loss.eval({input_z: batch_z, input_attack_remains: batch_attack_remains})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g),\n",
    "              \"Time elapsed: {:.4f}\".format(time.time() - start)\n",
    "             )    \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "    print('Total training time:', time.time() - total_start)\n",
    "    \n",
    "    final_benign_test_score = sess.run(d_model_benign, feed_dict={input_benign: benign_test})\n",
    "    final_attack_test_score = sess.run(d_model_benign, feed_dict={input_benign: attack_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss\n",
    "\n",
    "Here we'll check out the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator')\n",
    "plt.plot(losses.T[1], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test if discriminator is fooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy before training')\n",
    "print('Discriminator benign test accuracy:', (init_benign_test_score <= 0.5).sum() / benign_test.shape[0])\n",
    "print('Discriminator attack test accuracy:', (init_attack_test_score > 0.5).sum() / attack_test.shape[0])\n",
    "print('Discriminator total test accuracy:',\n",
    "      ((init_benign_test_score <= 0.5).sum() + (init_attack_test_score > 0.5).sum()) / (benign_test.shape[0] + attack_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy after training')\n",
    "print('Discriminator benign test accuracy:', (final_benign_test_score <= 0.5).sum() / benign_test.shape[0])\n",
    "print('Discriminator attack test accuracy:', (final_attack_test_score > 0.5).sum() / attack_test.shape[0])\n",
    "print('Discriminator total test accuracy:',\n",
    "      ((final_benign_test_score <= 0.5).sum() + (final_attack_test_score > 0.5).sum()) / (benign_test.shape[0] + attack_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def split_to_two_nearest_factor(x):\n",
    "    sqrt_x = int(math.sqrt(x))\n",
    "    i = sqrt_x\n",
    "    while x % i != 0:\n",
    "        i -= 1\n",
    "    return (i, x // i)\n",
    "fig_size = split_to_two_nearest_factor(input_size)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fooled = 0\n",
    "\n",
    "# fig=plt.figure(figsize=(fig_size[0] * 5, fig_size[1] * 5))\n",
    "# columns = 2\n",
    "# rows = len(original_flows)\n",
    "# for i in range(rows):\n",
    "#     original_img = original_flows[i][0]\n",
    "#     generated_img = generated_flows[i][0]\n",
    "#     ax_original = fig.add_subplot(rows, columns, i * 2 + 1)\n",
    "#     ax_original.set_title('Epoch {epoch}'.format(epoch=i + 1))\n",
    "#     plt.imshow(original_img.reshape(fig_size[0],fig_size[1]), cmap='gray')\n",
    "#     ax_generated = fig.add_subplot(rows, columns, i * 2 + 2)\n",
    "#     plt.imshow(generated_img.reshape(fig_size[0],fig_size[1]), cmap='gray')\n",
    "#     if attack_scores[i][0] <= 0.5:\n",
    "#         ax_generated.set_title('Fooled:' + str(attack_scores[i][0]))\n",
    "#         fooled += 1\n",
    "#     else:\n",
    "#         ax_generated.set_title('Just close:' + str(attack_scores[i][0]))\n",
    "# plt.show()\n",
    "# print('Fooled rate:', fooled / len(original_flows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate attack flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: generate attack flows for Kaihua's discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
    "                                  origin=test_url)\n",
    "\n",
    "test_dataset = tf.data.TextLineDataset(test_fp)\n",
    "test_dataset = test_dataset.skip(1)             # skip header row\n",
    "test_dataset = test_dataset.map(parse_csv)      # parse each row with the funcition created earlier\n",
    "test_dataset = test_dataset.shuffle(1000)       # randomize\n",
    "test_dataset = test_dataset.batch(32)           # use the same batch size as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n",
    "  test_accuracy(prediction, y)\n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"]\n",
    "\n",
    "predict_dataset = tf.convert_to_tensor([\n",
    "    [5.1, 3.3, 1.7, 0.5,],\n",
    "    [5.9, 3.0, 4.2, 1.5,],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "predictions = model(predict_dataset)\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "  class_idx = tf.argmax(logits).numpy()\n",
    "  name = class_ids[class_idx]\n",
    "  print(\"Example {} prediction: {}\".format(i, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
