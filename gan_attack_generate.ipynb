{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "In this notebook, we'll be building a generative adversarial network (GAN) trained on the network flow dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, models\n",
    "reload(utils)\n",
    "reload(models)\n",
    "from models import Generator, Discriminator\n",
    "from utils import max_norm, parse_feature_label, train_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出的summary的保存目录:不要改\n",
    "OUTPUT_DIR = 'SUMMARY/'\n",
    "\n",
    "# 输出的模型的保存目录:不要改\n",
    "CHECKPOINT_DIR = 'CHECKPOINT/'\n",
    "\n",
    "# 相关feature:不太需要改应该\n",
    "RELEVANT_FEATURES = [' Source Port', ' Destination Port', ' Flow Duration', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', 'Bwd Packet Length Max', ' Bwd Packet Length Min', 'Flow Bytes/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Fwd Header Length', ' Bwd Packets/s', ' Packet Length Mean', ' ACK Flag Count', ' Down/Up Ratio', ' Avg Fwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' act_data_pkt_fwd', ' Active Std', ' Active Min', ' Idle Max']\n",
    "\n",
    "# Label的名字:不太需要改应该\n",
    "LABEL_NAME = ' Label'\n",
    "\n",
    "# 记录summary的频率:不太需要改应该\n",
    "LOG_INTERVAL = 100\n",
    "\n",
    "# 数据集路径\n",
    "IDS_DATASET = os.path.join('data', 'ids2017_sampled.csv')\n",
    "\n",
    "# benign flow的label\n",
    "BENIGN_LABEL = 0\n",
    "\n",
    "# attack flow的label\n",
    "ATTACK_LABEL = 2\n",
    "\n",
    "# training dataset占总dataset的比例\n",
    "TRAIN_FRAC = 0.3\n",
    "\n",
    "# 要修改的feature的数量\n",
    "FEATURE_NUM_MODIFIED = 2\n",
    "\n",
    "# learning rate\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# 训练的epochs数\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow ID, Source IP, Source Port, Destination IP, Destination Port, Protocol, Timestamp, Flow Duration, Total Fwd Packets, Total Backward Packets,Total Length of Fwd Packets, Total Length of Bwd Packets, Fwd Packet Length Max, Fwd Packet Length Min, Fwd Packet Length Mean, Fwd Packet Length Std,Bwd Packet Length Max, Bwd Packet Length Min, Bwd Packet Length Mean, Bwd Packet Length Std,Flow Bytes/s, Flow Packets/s, Flow IAT Mean, Flow IAT Std, Flow IAT Max, Flow IAT Min,Fwd IAT Total, Fwd IAT Mean, Fwd IAT Std, Fwd IAT Max, Fwd IAT Min,Bwd IAT Total, Bwd IAT Mean, Bwd IAT Std, Bwd IAT Max, Bwd IAT Min,Fwd PSH Flags, Bwd PSH Flags, Fwd URG Flags, Bwd URG Flags, Fwd Header Length, Bwd Header Length,Fwd Packets/s, Bwd Packets/s, Min Packet Length, Max Packet Length, Packet Length Mean, Packet Length Std, Packet Length Variance,FIN Flag Count, SYN Flag Count, RST Flag Count, PSH Flag Count, ACK Flag Count, URG Flag Count, CWE Flag Count, ECE Flag Count, Down/Up Ratio, Average Packet Size, Avg Fwd Segment Size, Avg Bwd Segment Size, Fwd Header Length.1,Fwd Avg Bytes/Bulk, Fwd Avg Packets/Bulk, Fwd Avg Bulk Rate, Bwd Avg Bytes/Bulk, Bwd Avg Packets/Bulk,Bwd Avg Bulk Rate,Subflow Fwd Packets, Subflow Fwd Bytes, Subflow Bwd Packets, Subflow Bwd Bytes,Init_Win_bytes_forward, Init_Win_bytes_backward, act_data_pkt_fwd, min_seg_size_forward,Active Mean, Active Std, Active Max, Active Min,Idle Mean, Idle Std, Idle Max, Idle Min, Label\r\n",
      "214102,192.168.10.8,50305,23.194.108.67,80,6,5/7/2017 9:35,5559809,3,1,12,0.0,6,0,4.0,3.464101615,0,0,0.0,0.0,2.158347526,0.719449175,1853269.667,3189322.073,5535956.0,39.0,5559809.0,2779904.5,3897645.41,5535956.0,23853.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,72,32,0.539586881,0.179862294,0,6,2.4,3.286335345,10.8,0,0,0,1,0,0,0,0,0,3.0,4.0,0.0,72,0,0,0,0,0,0,3,12,1,0,8192,29200,2,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "36502,172.217.11.3,80,192.168.10.8,49917,6,5/7/2017 9:31,18,1,1,6,6.0,6,6,6.0,0.0,6,6,6.0,0.0,666666.6667,111111.1111,18.0,0.0,18.0,18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,20,20,55555.55556,55555.55556,6,6,6.0,0.0,0.0,0,0,0,0,1,1,0,0,1,9.0,6.0,6.0,20,0,0,0,0,0,0,1,6,1,6,343,16560,0,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "24975,192.168.10.50,80,172.16.0.1,51484,6,5/7/2017 11:00,12859,1,3,0,18.0,0,0,0.0,0.0,6,6,6.0,0.0,1399.797807,311.0661793,4286.333333,6734.794825,12049.0,1.0,0.0,0.0,0.0,0.0,0.0,12050.0,6025.0,8519.2225,12049.0,1.0,0,0,0,0,32,60,77.76654483,233.2996345,0,6,3.6,3.286335345,10.8,0,0,0,0,1,0,0,0,3,4.5,0.0,6.0,32,0,0,0,0,0,0,1,0,3,18,235,0,0,32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "151291,192.168.10.19,27610,192.168.10.3,53,17,5/7/2017 3:08,231,2,2,62,94.0,31,31,31.0,0.0,47,47,47.0,0.0,675324.6753,17316.01732,77.0,91.27978966,179.0,3.0,3.0,3.0,0.0,3.0,3.0,49.0,49.0,0.0,49.0,49.0,0,0,0,0,40,40,8658.008658,8658.008658,31,47,37.4,8.76356092,76.8,0,0,0,0,0,0,0,0,1,46.75,31.0,47.0,40,0,0,0,0,0,0,2,62,2,94,-1,-1,1,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n"
     ]
    }
   ],
   "source": [
    "# quick view of dataset\n",
    "!head -n5 {IDS_DATASET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(IDS_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant features and label name\n",
    "df = df[RELEVANT_FEATURES + [LABEL_NAME]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bengin and attack flows we want\n",
    "benign_df, attack_df = df[(df[LABEL_NAME] == BENIGN_LABEL)], df[(df[LABEL_NAME] == ATTACK_LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# rewrite label values\n",
    "benign_df.loc[:, LABEL_NAME] = 0\n",
    "attack_df.loc[:, LABEL_NAME] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "benign, attack = benign_df.values, attack_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max normalization\n",
    "benign[:, :len(RELEVANT_FEATURES)] = max_norm(benign[:, :len(RELEVANT_FEATURES)])\n",
    "attack[:, :len(RELEVANT_FEATURES)] = max_norm(attack[:, :len(RELEVANT_FEATURES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# do train, test split separately on benign and attack\n",
    "benign_train, benign_test = train_test_split(benign, train_size=TRAIN_FRAC)\n",
    "attack_train, attack_test = train_test_split(attack, train_size=TRAIN_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat to get testing data\n",
    "test_np = np.concatenate([benign_test, attack_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to benign train dataset and attack train dataset\n",
    "benign_train_dataset, attack_train_dataset = tf.data.Dataset.from_tensor_slices(benign_train), tf.data.Dataset.from_tensor_slices(attack_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign example features: tf.Tensor(\n",
      "[7.74075883e-01 1.24760226e-03 4.48925052e-02 4.56819093e-03\n",
      " 2.55811052e-04 1.58476027e-01 0.00000000e+00 1.19549734e-02\n",
      " 3.65357181e-03 1.48063524e-02 4.12737115e-02 4.68018713e-08\n",
      " 4.30117500e-03 5.46759534e-04 7.08206345e-04 1.54237288e-06\n",
      " 4.41663417e-02 3.20820884e-03 2.05297597e-02 1.69491525e-08\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.10567687e-04\n",
      " 2.78477168e-06 4.55249622e-01 0.00000000e+00 1.42857143e-01\n",
      " 3.32997940e-02 5.10567687e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.29000429e-04 4.56819093e-03\n",
      " 1.25015259e-01 4.34971727e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00], shape=(41,), dtype=float64)\n",
      "benign example label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# deal with benign train dataset\n",
    "benign_train_dataset = benign_train_dataset.map(parse_feature_label)\n",
    "benign_train_dataset = benign_train_dataset.shuffle(buffer_size=benign_train.shape[0] * 5)  # randomize\n",
    "benign_train_dataset = benign_train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "benign_features, benign_label = iter(benign_train_dataset).next()\n",
    "print(\"benign example features:\", benign_features[0])\n",
    "print(\"benign example label:\", benign_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack example features: tf.Tensor(\n",
      "[3.37726757e-01 0.00000000e+00 9.73550970e-01 1.54320988e-01\n",
      " 1.00000000e+00 3.74644243e-01 0.00000000e+00 9.82431870e-06\n",
      " 2.27229637e-01 4.70989761e-01 8.44915253e-01 1.10229277e-04\n",
      " 9.74789916e-01 4.91094141e-01 5.90236686e-01 3.84172109e-05\n",
      " 9.74789916e-01 1.89215686e-01 8.12714777e-01 4.60784314e-07\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.94230769e-01\n",
      " 6.04510190e-08 5.32086707e-01 0.00000000e+00 5.00000000e-01\n",
      " 1.75438596e-01 3.94230769e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.16666667e-01 1.54320988e-01\n",
      " 0.00000000e+00 3.33333333e-01 0.00000000e+00 3.44123615e-01\n",
      " 8.44915254e-01], shape=(41,), dtype=float64)\n",
      "attack example label: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# deal with attack train dataset\n",
    "attack_train_dataset = attack_train_dataset.map(parse_feature_label)\n",
    "attack_train_dataset = attack_train_dataset.shuffle(buffer_size=attack_train.shape[0] * 5)  # randomize\n",
    "attack_train_dataset = attack_train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "attack_features, attack_label = iter(attack_train_dataset).next()\n",
    "print(\"attack example features:\", attack_features[0])\n",
    "print(\"attack example label:\", attack_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_shape=len(RELEVANT_FEATURES), output_shape=FEATURE_NUM_MODIFIED)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Tensorflow Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.eager.python.checkpointable_utils.InitializationOnlyStatus at 0x2af817cb6278>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_counter = tf.train.get_or_create_global_step()\n",
    "summary_writer = tf.contrib.summary.create_file_writer(\n",
    "      OUTPUT_DIR, flush_millis=1000)\n",
    "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, 'ckpt')\n",
    "latest_cpkt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "if latest_cpkt:\n",
    "    print('Using latest checkpoint at ' + latest_cpkt)\n",
    "model_objects = {\n",
    "    'generator': generator,\n",
    "    'discriminator': discriminator,\n",
    "    'generator_optimizer': generator_optimizer,\n",
    "    'discriminator_optimizer': discriminator_optimizer,\n",
    "    'step_counter': step_counter\n",
    "}\n",
    "checkpoint = tfe.Checkpoint(**model_objects)\n",
    "# Restore variables on creation if a checkpoint exists.\n",
    "checkpoint.restore(latest_cpkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train time for epoch #1 (step 15): 1.396186\n",
      "\n",
      "Train time for epoch #2 (step 30): 1.259199\n",
      "\n",
      "Train time for epoch #3 (step 45): 1.293359\n",
      "\n",
      "Train time for epoch #4 (step 60): 1.380395\n",
      "\n",
      "Train time for epoch #5 (step 75): 1.286117\n",
      "\n",
      "Train time for epoch #6 (step 90): 1.241150\n",
      "\n",
      "Train time for epoch #7 (step 105): 1.300222\n",
      "\n",
      "Train time for epoch #8 (step 120): 1.239007\n",
      "\n",
      "Train time for epoch #9 (step 135): 1.161514\n",
      "\n",
      "Train time for epoch #10 (step 150): 1.287961\n",
      "\n",
      "Train time for epoch #11 (step 165): 1.327828\n",
      "\n",
      "Train time for epoch #12 (step 180): 1.202986\n",
      "\n",
      "Train time for epoch #13 (step 195): 1.290864\n",
      "\n",
      "Train time for epoch #14 (step 210): 1.241826\n",
      "\n",
      "Train time for epoch #15 (step 225): 1.132176\n",
      "\n",
      "Train time for epoch #16 (step 240): 1.208208\n",
      "\n",
      "Train time for epoch #17 (step 255): 1.298326\n",
      "\n",
      "Train time for epoch #18 (step 270): 1.252905\n",
      "\n",
      "Train time for epoch #19 (step 285): 1.265866\n",
      "\n",
      "Train time for epoch #20 (step 300): 1.246461\n",
      "\n",
      "Total training time for 20 epoch(s) is 25.76100778579712\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    train_start = time.time()\n",
    "    for _ in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        with summary_writer.as_default():\n",
    "            train_one_epoch(benign_dataset=benign_train_dataset,\n",
    "                        attack_dataset=attack_train_dataset,\n",
    "                        log_interval=LOG_INTERVAL,\n",
    "                        modified_feature_num=FEATURE_NUM_MODIFIED,\n",
    "                        **model_objects)\n",
    "        end = time.time()\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "        print('\\nTrain time for epoch #%d (step %d): %f' %\n",
    "            (checkpoint.save_counter.numpy(),\n",
    "             checkpoint.step_counter.numpy(),\n",
    "             end - start))\n",
    "    print('\\nTotal training time for {epoch} epoch(s) is {second}'.format(\n",
    "        second=time.time() - train_start,\n",
    "        epoch=EPOCHS\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
