{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "In this notebook, we'll be building a generative adversarial network (GAN) trained on the network flow dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, models\n",
    "reload(utils)\n",
    "reload(models)\n",
    "from models import Generator, Discriminator\n",
    "from utils import max_norm, parse_feature_label, train_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出的summary的保存目录:不要改\n",
    "OUTPUT_DIR = 'SUMMARY/'\n",
    "\n",
    "# 输出的模型的保存目录:不要改\n",
    "CHECKPOINT_DIR = 'CHECKPOINT/'\n",
    "\n",
    "# 保存data的目录:不要改\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# 相关feature:不太需要改应该\n",
    "RELEVANT_FEATURES = [' Source Port', ' Destination Port', ' Flow Duration', 'Total Length of Fwd Packets', ' Total Length of Bwd Packets', 'Bwd Packet Length Max', ' Bwd Packet Length Min', 'Flow Bytes/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Fwd Header Length', ' Bwd Packets/s', ' Packet Length Mean', ' ACK Flag Count', ' Down/Up Ratio', ' Avg Fwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Bwd Avg Bytes/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', 'Init_Win_bytes_forward', ' act_data_pkt_fwd', ' Active Std', ' Active Min', ' Idle Max']\n",
    "\n",
    "# Label的名字:不太需要改应该\n",
    "LABEL_NAME = ' Label'\n",
    "\n",
    "# 记录summary的频率:不太需要改应该\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "# 数据集路径\n",
    "IDS_DATASET = os.path.join('data', 'ids2017_sampled.csv')\n",
    "\n",
    "# benign flow的label\n",
    "BENIGN_LABEL = 0\n",
    "\n",
    "# attack flow的label\n",
    "ATTACK_LABEL = 2\n",
    "\n",
    "# training dataset占总dataset的比例\n",
    "TRAIN_FRAC = 0.3\n",
    "\n",
    "# 要修改的feature的数量\n",
    "FEATURE_NUM_MODIFIED = 2\n",
    "\n",
    "# learning rate\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# 训练的epochs数\n",
    "EPOCHS = 200\n",
    "\n",
    "# 是否随机选择feature\n",
    "RANDOM_SELECT_FEATURE=False\n",
    "\n",
    "## 提示：\n",
    "## 训练结束后，把SUMMARY和CHECKPOINT目录重命名后保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow ID, Source IP, Source Port, Destination IP, Destination Port, Protocol, Timestamp, Flow Duration, Total Fwd Packets, Total Backward Packets,Total Length of Fwd Packets, Total Length of Bwd Packets, Fwd Packet Length Max, Fwd Packet Length Min, Fwd Packet Length Mean, Fwd Packet Length Std,Bwd Packet Length Max, Bwd Packet Length Min, Bwd Packet Length Mean, Bwd Packet Length Std,Flow Bytes/s, Flow Packets/s, Flow IAT Mean, Flow IAT Std, Flow IAT Max, Flow IAT Min,Fwd IAT Total, Fwd IAT Mean, Fwd IAT Std, Fwd IAT Max, Fwd IAT Min,Bwd IAT Total, Bwd IAT Mean, Bwd IAT Std, Bwd IAT Max, Bwd IAT Min,Fwd PSH Flags, Bwd PSH Flags, Fwd URG Flags, Bwd URG Flags, Fwd Header Length, Bwd Header Length,Fwd Packets/s, Bwd Packets/s, Min Packet Length, Max Packet Length, Packet Length Mean, Packet Length Std, Packet Length Variance,FIN Flag Count, SYN Flag Count, RST Flag Count, PSH Flag Count, ACK Flag Count, URG Flag Count, CWE Flag Count, ECE Flag Count, Down/Up Ratio, Average Packet Size, Avg Fwd Segment Size, Avg Bwd Segment Size, Fwd Header Length.1,Fwd Avg Bytes/Bulk, Fwd Avg Packets/Bulk, Fwd Avg Bulk Rate, Bwd Avg Bytes/Bulk, Bwd Avg Packets/Bulk,Bwd Avg Bulk Rate,Subflow Fwd Packets, Subflow Fwd Bytes, Subflow Bwd Packets, Subflow Bwd Bytes,Init_Win_bytes_forward, Init_Win_bytes_backward, act_data_pkt_fwd, min_seg_size_forward,Active Mean, Active Std, Active Max, Active Min,Idle Mean, Idle Std, Idle Max, Idle Min, Label\r\n",
      "214102,192.168.10.8,50305,23.194.108.67,80,6,5/7/2017 9:35,5559809,3,1,12,0.0,6,0,4.0,3.464101615,0,0,0.0,0.0,2.158347526,0.719449175,1853269.667,3189322.073,5535956.0,39.0,5559809.0,2779904.5,3897645.41,5535956.0,23853.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,72,32,0.539586881,0.179862294,0,6,2.4,3.286335345,10.8,0,0,0,1,0,0,0,0,0,3.0,4.0,0.0,72,0,0,0,0,0,0,3,12,1,0,8192,29200,2,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "36502,172.217.11.3,80,192.168.10.8,49917,6,5/7/2017 9:31,18,1,1,6,6.0,6,6,6.0,0.0,6,6,6.0,0.0,666666.6667,111111.1111,18.0,0.0,18.0,18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0,0,0,0,20,20,55555.55556,55555.55556,6,6,6.0,0.0,0.0,0,0,0,0,1,1,0,0,1,9.0,6.0,6.0,20,0,0,0,0,0,0,1,6,1,6,343,16560,0,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "24975,192.168.10.50,80,172.16.0.1,51484,6,5/7/2017 11:00,12859,1,3,0,18.0,0,0,0.0,0.0,6,6,6.0,0.0,1399.797807,311.0661793,4286.333333,6734.794825,12049.0,1.0,0.0,0.0,0.0,0.0,0.0,12050.0,6025.0,8519.2225,12049.0,1.0,0,0,0,0,32,60,77.76654483,233.2996345,0,6,3.6,3.286335345,10.8,0,0,0,0,1,0,0,0,3,4.5,0.0,6.0,32,0,0,0,0,0,0,1,0,3,18,235,0,0,32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n",
      "151291,192.168.10.19,27610,192.168.10.3,53,17,5/7/2017 3:08,231,2,2,62,94.0,31,31,31.0,0.0,47,47,47.0,0.0,675324.6753,17316.01732,77.0,91.27978966,179.0,3.0,3.0,3.0,0.0,3.0,3.0,49.0,49.0,0.0,49.0,49.0,0,0,0,0,40,40,8658.008658,8658.008658,31,47,37.4,8.76356092,76.8,0,0,0,0,0,0,0,0,1,46.75,31.0,47.0,40,0,0,0,0,0,0,2,62,2,94,-1,-1,1,20,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0\r\n"
     ]
    }
   ],
   "source": [
    "# quick view of dataset\n",
    "!head -n5 {IDS_DATASET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(IDS_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant features and label name\n",
    "df = df[RELEVANT_FEATURES + [LABEL_NAME]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract bengin and attack flows we want\n",
    "benign_df, attack_df = df[(df[LABEL_NAME] == BENIGN_LABEL)], df[(df[LABEL_NAME] == ATTACK_LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# rewrite label values\n",
    "benign_df.loc[:, LABEL_NAME] = 0\n",
    "attack_df.loc[:, LABEL_NAME] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "benign, attack = benign_df.values, attack_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max normalization\n",
    "benign[:, :len(RELEVANT_FEATURES)] = max_norm(benign[:, :len(RELEVANT_FEATURES)])\n",
    "attack[:, :len(RELEVANT_FEATURES)] = max_norm(attack[:, :len(RELEVANT_FEATURES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# do train, test split separately on benign and attack\n",
    "benign_train, benign_test = train_test_split(benign, train_size=TRAIN_FRAC)\n",
    "attack_train, attack_test = train_test_split(attack, train_size=TRAIN_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat to get testing data\n",
    "test_np = np.concatenate([benign_test, attack_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to benign train dataset and attack train dataset\n",
    "benign_train_dataset, attack_train_dataset = tf.data.Dataset.from_tensor_slices(benign_train), tf.data.Dataset.from_tensor_slices(attack_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign example features: tf.Tensor(\n",
      "[9.50474650e-01 8.26536500e-04 3.43375506e-06 2.82636273e-04\n",
      " 1.05169340e-06 5.05136986e-03 1.01549053e-01 1.21993777e-02\n",
      " 6.42745700e-06 0.00000000e+00 3.49152539e-06 6.42745700e-06\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 8.50946146e-05\n",
      " 2.43309002e-03 3.65607665e-02 0.00000000e+00 1.42857143e-01\n",
      " 1.85424752e-02 8.50946146e-05 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.82636273e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00], shape=(41,), dtype=float64)\n",
      "benign example label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# deal with benign train dataset\n",
    "benign_train_dataset = benign_train_dataset.map(parse_feature_label)\n",
    "benign_train_dataset = benign_train_dataset.shuffle(buffer_size=benign_train.shape[0] * 5)  # randomize\n",
    "benign_train_dataset = benign_train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "benign_features, benign_label = iter(benign_train_dataset).next()\n",
    "print(\"benign example features:\", benign_features[0])\n",
    "print(\"benign example label:\", benign_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack example features: tf.Tensor(\n",
      "[8.98951247e-01 0.00000000e+00 2.52223495e-08 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.92857154e-08 0.00000000e+00 2.54237290e-08 2.75573192e-04\n",
      " 2.52100842e-08 8.90585253e-08 0.00000000e+00 1.53668844e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.76923077e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 5.76923077e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 8.59589041e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00], shape=(41,), dtype=float64)\n",
      "attack example label: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# deal with attack train dataset\n",
    "attack_train_dataset = attack_train_dataset.map(parse_feature_label)\n",
    "attack_train_dataset = attack_train_dataset.shuffle(buffer_size=attack_train.shape[0] * 5)  # randomize\n",
    "attack_train_dataset = attack_train_dataset.batch(100) # make batch\n",
    "\n",
    "# View a single example entry from a batch\n",
    "attack_features, attack_label = iter(attack_train_dataset).next()\n",
    "print(\"attack example features:\", attack_features[0])\n",
    "print(\"attack example label:\", attack_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_shape=len(RELEVANT_FEATURES), output_shape=FEATURE_NUM_MODIFIED)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Tensorflow Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest checkpoint at CHECKPOINT/ckpt-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.eager.python.checkpointable_utils.CheckpointLoadStatus at 0x2aacebaa77b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_counter = tf.train.get_or_create_global_step()\n",
    "summary_writer = tf.contrib.summary.create_file_writer(\n",
    "      OUTPUT_DIR, flush_millis=1000)\n",
    "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, 'ckpt')\n",
    "latest_cpkt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "if latest_cpkt:\n",
    "    print('Using latest checkpoint at ' + latest_cpkt)\n",
    "model_objects = {\n",
    "    'generator': generator,\n",
    "    'discriminator': discriminator,\n",
    "    'generator_optimizer': generator_optimizer,\n",
    "    'discriminator_optimizer': discriminator_optimizer,\n",
    "    'step_counter': step_counter\n",
    "}\n",
    "checkpoint = tfe.Checkpoint(**model_objects)\n",
    "# Restore variables on creation if a checkpoint exists.\n",
    "checkpoint.restore(latest_cpkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #10\tAverage Generator Loss: 9.638695\tAverage Discriminator Loss: 0.237832\n",
      "\n",
      "Train time for epoch #8 (step 8): 1.684991\n",
      "Batch #10\tAverage Generator Loss: 9.067811\tAverage Discriminator Loss: 0.880001\n",
      "\n",
      "Train time for epoch #9 (step 9): 1.203411\n",
      "Batch #10\tAverage Generator Loss: 11.318812\tAverage Discriminator Loss: 0.872652\n",
      "\n",
      "Train time for epoch #10 (step 10): 1.250474\n",
      "Batch #10\tAverage Generator Loss: 13.696983\tAverage Discriminator Loss: 0.541398\n",
      "\n",
      "Train time for epoch #11 (step 11): 1.282816\n",
      "Batch #10\tAverage Generator Loss: 11.837294\tAverage Discriminator Loss: 0.505805\n",
      "\n",
      "Train time for epoch #12 (step 12): 1.270658\n",
      "Batch #10\tAverage Generator Loss: 12.482833\tAverage Discriminator Loss: 0.340769\n",
      "\n",
      "Train time for epoch #13 (step 13): 1.342939\n",
      "Batch #10\tAverage Generator Loss: 16.936109\tAverage Discriminator Loss: 0.250954\n",
      "\n",
      "Train time for epoch #14 (step 14): 1.291723\n",
      "Batch #10\tAverage Generator Loss: 13.717525\tAverage Discriminator Loss: 0.612908\n",
      "\n",
      "Train time for epoch #15 (step 15): 1.356047\n",
      "Batch #10\tAverage Generator Loss: 15.768079\tAverage Discriminator Loss: 0.393826\n",
      "\n",
      "Train time for epoch #16 (step 16): 1.269020\n",
      "Batch #10\tAverage Generator Loss: 16.199646\tAverage Discriminator Loss: 0.471440\n",
      "\n",
      "Train time for epoch #17 (step 17): 1.264752\n",
      "Batch #10\tAverage Generator Loss: 14.047297\tAverage Discriminator Loss: 0.273958\n",
      "\n",
      "Train time for epoch #18 (step 18): 1.225831\n",
      "Batch #10\tAverage Generator Loss: 14.196453\tAverage Discriminator Loss: 0.261604\n",
      "\n",
      "Train time for epoch #19 (step 19): 1.325148\n",
      "Batch #10\tAverage Generator Loss: 15.973139\tAverage Discriminator Loss: 0.292102\n",
      "\n",
      "Train time for epoch #20 (step 20): 1.352185\n",
      "Batch #10\tAverage Generator Loss: 13.565648\tAverage Discriminator Loss: 0.238365\n",
      "\n",
      "Train time for epoch #21 (step 21): 1.234664\n",
      "Batch #10\tAverage Generator Loss: 15.691610\tAverage Discriminator Loss: 0.301419\n",
      "\n",
      "Train time for epoch #22 (step 22): 1.331509\n",
      "Batch #10\tAverage Generator Loss: 11.702929\tAverage Discriminator Loss: 0.562545\n",
      "\n",
      "Train time for epoch #23 (step 23): 1.283915\n",
      "Batch #10\tAverage Generator Loss: 13.421958\tAverage Discriminator Loss: 0.269620\n",
      "\n",
      "Train time for epoch #24 (step 24): 1.244610\n",
      "Batch #10\tAverage Generator Loss: 12.267635\tAverage Discriminator Loss: 0.242217\n",
      "\n",
      "Train time for epoch #25 (step 25): 1.328156\n",
      "Batch #10\tAverage Generator Loss: 17.470873\tAverage Discriminator Loss: 0.193146\n",
      "\n",
      "Train time for epoch #26 (step 26): 1.290002\n",
      "Batch #10\tAverage Generator Loss: 21.143569\tAverage Discriminator Loss: 0.174431\n",
      "\n",
      "Train time for epoch #27 (step 27): 1.335510\n",
      "Batch #10\tAverage Generator Loss: 16.756572\tAverage Discriminator Loss: 0.167996\n",
      "\n",
      "Train time for epoch #28 (step 28): 1.215022\n",
      "Batch #10\tAverage Generator Loss: 20.968634\tAverage Discriminator Loss: 0.178394\n",
      "\n",
      "Train time for epoch #29 (step 29): 1.312271\n",
      "Batch #10\tAverage Generator Loss: 18.924377\tAverage Discriminator Loss: 0.129417\n",
      "\n",
      "Train time for epoch #30 (step 30): 1.383369\n",
      "Batch #10\tAverage Generator Loss: 18.208134\tAverage Discriminator Loss: 0.237082\n",
      "\n",
      "Train time for epoch #31 (step 31): 1.257282\n",
      "Batch #10\tAverage Generator Loss: 19.928110\tAverage Discriminator Loss: 0.099551\n",
      "\n",
      "Train time for epoch #32 (step 32): 1.248766\n",
      "Batch #10\tAverage Generator Loss: 22.725324\tAverage Discriminator Loss: 0.174997\n",
      "\n",
      "Train time for epoch #33 (step 33): 1.260307\n",
      "Batch #10\tAverage Generator Loss: 23.074471\tAverage Discriminator Loss: 0.128567\n",
      "\n",
      "Train time for epoch #34 (step 34): 1.237277\n",
      "Batch #10\tAverage Generator Loss: 23.639446\tAverage Discriminator Loss: 0.133217\n",
      "\n",
      "Train time for epoch #35 (step 35): 1.298203\n",
      "Batch #10\tAverage Generator Loss: 25.034123\tAverage Discriminator Loss: 0.099917\n",
      "\n",
      "Train time for epoch #36 (step 36): 1.256975\n",
      "Batch #10\tAverage Generator Loss: 25.411414\tAverage Discriminator Loss: 0.219035\n",
      "\n",
      "Train time for epoch #37 (step 37): 1.185959\n",
      "Batch #10\tAverage Generator Loss: 28.334647\tAverage Discriminator Loss: 0.080576\n",
      "\n",
      "Train time for epoch #38 (step 38): 1.275310\n",
      "Batch #10\tAverage Generator Loss: 25.530690\tAverage Discriminator Loss: 0.057186\n",
      "\n",
      "Train time for epoch #39 (step 39): 1.263601\n",
      "Batch #10\tAverage Generator Loss: 22.601277\tAverage Discriminator Loss: 0.324830\n",
      "\n",
      "Train time for epoch #40 (step 40): 1.246929\n",
      "Batch #10\tAverage Generator Loss: 24.251132\tAverage Discriminator Loss: 0.204792\n",
      "\n",
      "Train time for epoch #41 (step 41): 1.248902\n",
      "Batch #10\tAverage Generator Loss: 27.789726\tAverage Discriminator Loss: 0.159501\n",
      "\n",
      "Train time for epoch #42 (step 42): 1.303286\n",
      "Batch #10\tAverage Generator Loss: 25.692179\tAverage Discriminator Loss: 0.234005\n",
      "\n",
      "Train time for epoch #43 (step 43): 1.347457\n",
      "Batch #10\tAverage Generator Loss: 25.765832\tAverage Discriminator Loss: 0.116767\n",
      "\n",
      "Train time for epoch #44 (step 44): 1.250208\n",
      "Batch #10\tAverage Generator Loss: 31.418656\tAverage Discriminator Loss: 0.125658\n",
      "\n",
      "Train time for epoch #45 (step 45): 1.288702\n",
      "Batch #10\tAverage Generator Loss: 26.521353\tAverage Discriminator Loss: 0.112419\n",
      "\n",
      "Train time for epoch #46 (step 46): 1.279728\n",
      "Batch #10\tAverage Generator Loss: 27.665785\tAverage Discriminator Loss: 0.142272\n",
      "\n",
      "Train time for epoch #47 (step 47): 1.355745\n",
      "Batch #10\tAverage Generator Loss: 31.104871\tAverage Discriminator Loss: 0.083168\n",
      "\n",
      "Train time for epoch #48 (step 48): 1.288228\n",
      "Batch #10\tAverage Generator Loss: 32.774511\tAverage Discriminator Loss: 0.069505\n",
      "\n",
      "Train time for epoch #49 (step 49): 1.205247\n",
      "Batch #10\tAverage Generator Loss: 32.299052\tAverage Discriminator Loss: 0.265487\n",
      "\n",
      "Train time for epoch #50 (step 50): 1.264638\n",
      "Batch #10\tAverage Generator Loss: 31.218950\tAverage Discriminator Loss: 0.221289\n",
      "\n",
      "Train time for epoch #51 (step 51): 1.218337\n",
      "Batch #10\tAverage Generator Loss: 33.230926\tAverage Discriminator Loss: 0.109475\n",
      "\n",
      "Train time for epoch #52 (step 52): 1.264111\n",
      "Batch #10\tAverage Generator Loss: 32.517121\tAverage Discriminator Loss: 0.077313\n",
      "\n",
      "Train time for epoch #53 (step 53): 1.285227\n",
      "Batch #10\tAverage Generator Loss: 31.941094\tAverage Discriminator Loss: 0.093481\n",
      "\n",
      "Train time for epoch #54 (step 54): 1.228743\n",
      "Batch #10\tAverage Generator Loss: 29.500530\tAverage Discriminator Loss: 0.208240\n",
      "\n",
      "Train time for epoch #55 (step 55): 1.224968\n",
      "Batch #10\tAverage Generator Loss: 33.629631\tAverage Discriminator Loss: 0.112660\n",
      "\n",
      "Train time for epoch #56 (step 56): 1.351910\n",
      "Batch #10\tAverage Generator Loss: 31.996339\tAverage Discriminator Loss: 0.113154\n",
      "\n",
      "Train time for epoch #57 (step 57): 1.143890\n",
      "Batch #10\tAverage Generator Loss: 37.521619\tAverage Discriminator Loss: 0.110071\n",
      "\n",
      "Train time for epoch #58 (step 58): 1.314255\n",
      "Batch #10\tAverage Generator Loss: 34.865016\tAverage Discriminator Loss: 0.080937\n",
      "\n",
      "Train time for epoch #59 (step 59): 1.246332\n",
      "Batch #10\tAverage Generator Loss: 39.300475\tAverage Discriminator Loss: 0.082260\n",
      "\n",
      "Train time for epoch #60 (step 60): 1.286752\n",
      "Batch #10\tAverage Generator Loss: 42.826935\tAverage Discriminator Loss: 0.035757\n",
      "\n",
      "Train time for epoch #61 (step 61): 1.251291\n",
      "Batch #10\tAverage Generator Loss: 46.122599\tAverage Discriminator Loss: 0.049660\n",
      "\n",
      "Train time for epoch #62 (step 62): 1.317742\n",
      "Batch #10\tAverage Generator Loss: 47.639700\tAverage Discriminator Loss: 0.092357\n",
      "\n",
      "Train time for epoch #63 (step 63): 1.288084\n",
      "Batch #10\tAverage Generator Loss: 45.926432\tAverage Discriminator Loss: 0.055545\n",
      "\n",
      "Train time for epoch #64 (step 64): 1.164619\n",
      "Batch #10\tAverage Generator Loss: 45.753703\tAverage Discriminator Loss: 0.160538\n",
      "\n",
      "Train time for epoch #65 (step 65): 1.236927\n",
      "Batch #10\tAverage Generator Loss: 46.928403\tAverage Discriminator Loss: 0.038134\n",
      "\n",
      "Train time for epoch #66 (step 66): 1.227706\n",
      "Batch #10\tAverage Generator Loss: 50.900629\tAverage Discriminator Loss: 0.030467\n",
      "\n",
      "Train time for epoch #67 (step 67): 1.272126\n",
      "Batch #10\tAverage Generator Loss: 46.592398\tAverage Discriminator Loss: 0.091286\n",
      "\n",
      "Train time for epoch #68 (step 68): 1.198715\n",
      "Batch #10\tAverage Generator Loss: 45.761636\tAverage Discriminator Loss: 0.142315\n",
      "\n",
      "Train time for epoch #69 (step 69): 1.267730\n",
      "Batch #10\tAverage Generator Loss: 48.565599\tAverage Discriminator Loss: 0.113408\n",
      "\n",
      "Train time for epoch #70 (step 70): 1.249694\n",
      "Batch #10\tAverage Generator Loss: 51.258058\tAverage Discriminator Loss: 0.063522\n",
      "\n",
      "Train time for epoch #71 (step 71): 1.262990\n",
      "Batch #10\tAverage Generator Loss: 43.386384\tAverage Discriminator Loss: 0.070090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train time for epoch #72 (step 72): 1.249829\n",
      "Batch #10\tAverage Generator Loss: 49.691627\tAverage Discriminator Loss: 0.061968\n",
      "\n",
      "Train time for epoch #73 (step 73): 1.256382\n",
      "Batch #10\tAverage Generator Loss: 50.048611\tAverage Discriminator Loss: 0.026237\n",
      "\n",
      "Train time for epoch #74 (step 74): 1.273839\n",
      "Batch #10\tAverage Generator Loss: 55.858282\tAverage Discriminator Loss: 0.016405\n",
      "\n",
      "Train time for epoch #75 (step 75): 1.151417\n",
      "Batch #10\tAverage Generator Loss: 55.689655\tAverage Discriminator Loss: 0.038453\n",
      "\n",
      "Train time for epoch #76 (step 76): 1.194993\n",
      "Batch #10\tAverage Generator Loss: 63.513253\tAverage Discriminator Loss: 0.022504\n",
      "\n",
      "Train time for epoch #77 (step 77): 1.235054\n",
      "Batch #10\tAverage Generator Loss: 62.972135\tAverage Discriminator Loss: 0.016289\n",
      "\n",
      "Train time for epoch #78 (step 78): 1.228962\n",
      "Batch #10\tAverage Generator Loss: 58.567912\tAverage Discriminator Loss: 0.023251\n",
      "\n",
      "Train time for epoch #79 (step 79): 1.235872\n",
      "Batch #10\tAverage Generator Loss: 53.619033\tAverage Discriminator Loss: 0.122608\n",
      "\n",
      "Train time for epoch #80 (step 80): 1.386177\n",
      "Batch #10\tAverage Generator Loss: 58.442720\tAverage Discriminator Loss: 0.038405\n",
      "\n",
      "Train time for epoch #81 (step 81): 1.235764\n",
      "Batch #10\tAverage Generator Loss: 61.944733\tAverage Discriminator Loss: 0.089187\n",
      "\n",
      "Train time for epoch #82 (step 82): 1.251106\n",
      "Batch #10\tAverage Generator Loss: 70.808976\tAverage Discriminator Loss: 0.023566\n",
      "\n",
      "Train time for epoch #83 (step 83): 1.245323\n",
      "Batch #10\tAverage Generator Loss: 62.283544\tAverage Discriminator Loss: 0.028228\n",
      "\n",
      "Train time for epoch #84 (step 84): 1.234751\n",
      "Batch #10\tAverage Generator Loss: 63.765284\tAverage Discriminator Loss: 0.011279\n",
      "\n",
      "Train time for epoch #85 (step 85): 1.261592\n",
      "Batch #10\tAverage Generator Loss: 63.858260\tAverage Discriminator Loss: 0.012769\n",
      "\n",
      "Train time for epoch #86 (step 86): 1.273255\n",
      "Batch #10\tAverage Generator Loss: 68.629803\tAverage Discriminator Loss: 0.018755\n",
      "\n",
      "Train time for epoch #87 (step 87): 1.314317\n",
      "Batch #10\tAverage Generator Loss: 75.453058\tAverage Discriminator Loss: 0.007982\n",
      "\n",
      "Train time for epoch #88 (step 88): 1.247304\n",
      "Batch #10\tAverage Generator Loss: 74.393098\tAverage Discriminator Loss: 0.007360\n",
      "\n",
      "Train time for epoch #89 (step 89): 1.241720\n",
      "Batch #10\tAverage Generator Loss: 76.094131\tAverage Discriminator Loss: 0.004285\n",
      "\n",
      "Train time for epoch #90 (step 90): 1.218579\n",
      "Batch #10\tAverage Generator Loss: 74.890633\tAverage Discriminator Loss: 0.068356\n",
      "\n",
      "Train time for epoch #91 (step 91): 1.200823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d1b5079b7b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mmodified_feature_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFEATURE_NUM_MODIFIED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mrandom_select_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SELECT_FEATURE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         **model_objects)\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sj2363/hsn/attack_generate/gan_attack_generate/utils.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(generator, discriminator, generator_optimizer, discriminator_optimizer, benign_dataset, attack_dataset, step_counter, log_interval, modified_feature_num, random_select_feature)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_feat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_attack_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# TODO: find how to use this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sj2363/hsn/attack_generate/gan_attack_generate/.env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   8167\u001b[0m         \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8168\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shrink_axis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8169\u001b[0;31m         shrink_axis_mask)\n\u001b[0m\u001b[1;32m   8170\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    train_start = time.time()\n",
    "    for _ in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        with summary_writer.as_default():\n",
    "            train_one_epoch(benign_dataset=benign_train_dataset,\n",
    "                        attack_dataset=attack_train_dataset,\n",
    "                        log_interval=LOG_INTERVAL,\n",
    "                        modified_feature_num=FEATURE_NUM_MODIFIED,\n",
    "                        random_select_feature=RANDOM_SELECT_FEATURE,\n",
    "                        **model_objects)\n",
    "        end = time.time()\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "        print('\\nTrain time for epoch #%d (step %d): %f' %\n",
    "            (checkpoint.save_counter.numpy(),\n",
    "             checkpoint.step_counter.numpy(),\n",
    "             end - start))\n",
    "    print('\\nTotal training time for {epoch} epoch(s) is {second}'.format(\n",
    "        second=time.time() - train_start,\n",
    "        epoch=EPOCHS\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(DATA_DIR, 'benign_train.npy'), benign_train)\n",
    "np.save(os.path.join(DATA_DIR, 'benign_test.npy'), benign_test)\n",
    "np.save(os.path.join(DATA_DIR, 'attack_train.npy'), attack_train)\n",
    "np.save(os.path.join(DATA_DIR, 'attack_test.npy'), attack_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_benign_train = np.load(os.path.join(DATA_DIR, 'benign_train.npy'))\n",
    "loaded_benign_test = np.load(os.path.join(DATA_DIR, 'benign_test.npy'))\n",
    "loaded_attack_train = np.load(os.path.join(DATA_DIR, 'attack_train.npy'))\n",
    "loaded_attack_test = np.load(os.path.join(DATA_DIR, 'attack_test.npy'))\n",
    "assert (loaded_benign_train == benign_train).all()\n",
    "assert (loaded_benign_test == benign_test).all()\n",
    "assert (loaded_attack_train == attack_train).all()\n",
    "assert (loaded_attack_test == attack_test).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
